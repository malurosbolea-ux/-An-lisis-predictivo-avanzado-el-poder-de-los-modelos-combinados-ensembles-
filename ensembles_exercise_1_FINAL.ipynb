{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Are ensembles better than their members?",
    "",
    "**Módulo:** Aprendizaje Automático y Aprendizaje Profundo",
    "",
    "**Alumna:** María Luisa Ros Bolea"
   ],
   "id": "0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Subida de datos a Colab",
    "",
    "Ejecuto esta celda, se abre un cuadro para seleccionar archivos. Selecciono TODOS los .train y .test a la vez (los 26 archivos) y se suben a la carpeta correcta."
   ],
   "id": "1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "os.makedirs('exercise_1_data', exist_ok=True)\n",
    "\n",
    "print(\"Selecciona todos los archivos .train y .test (los 26):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.train') or filename.endswith('.test'):\n",
    "        dest = os.path.join('exercise_1_data', filename)\n",
    "        with open(dest, 'wb') as f:\n",
    "            f.write(uploaded[filename])\n",
    "\n",
    "archivos = os.listdir('exercise_1_data')\n",
    "n = len(archivos)\n",
    "print(\"\")\n",
    "print(\"Archivos en exercise_1_data: \" + str(n))\n",
    "print(\"Todo listo.\")"
   ],
   "id": "2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fijar la semilla aleatoria",
    "",
    "Lo primero: fijo la semilla. Muchos algoritmos de ML tienen componentes aleatorios (inicialización de pesos, muestreo, particiones internas...) y sin semilla fija cada ejecución daría resultados distintos. Con `np.random.seed(42)` me aseguro de que todo sea reproducible. Que bastante caos tengo ya en mi vida como para añadir aleatoriedad a los experimentos."
   ],
   "id": "3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "print(\"Semilla fijada a 42. Reproducibilidad asegurada.\")"
   ],
   "id": "4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de nada, configuro las librerías y defino una paleta de colores pastel/rosa que voy a reutilizar en todos los gráficos del notebook. Así me ahorro repetir código y de paso queda todo coherente visualmente."
   ],
   "id": "5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuro el estilo visual para TODO el notebook\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.facecolor'] = '#FFF5F8'     # fondo rosa clarito\n",
    "plt.rcParams['figure.facecolor'] = '#FFFFFF'\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.color'] = '#D5A6BD'\n",
    "\n",
    "# Paleta rosa/pastel que reutilizo en todo\n",
    "ROSA4 = ['#F48FB1', '#CE93D8', '#90CAF9', '#A5D6A7']  # 4 base learners\n",
    "ROSA8 = ['#F48FB1', '#CE93D8', '#90CAF9', '#A5D6A7',  # base\n",
    "         '#EC407A', '#AB47BC', '#42A5F5', '#66BB6A']   # bagging (mas intensos)\n",
    "ROSA_SEQ = ['#FCE4EC', '#F8BBD0', '#F48FB1', '#EC407A', '#C2185B', '#880E4F']\n",
    "\n",
    "print(\"Estilo visual configurado.\")"
   ],
   "id": "6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspección de la carpeta de datos",
    "",
    "Reviso la carpeta `exercise_1_data`. Según el enunciado tengo 13 datasets, cada uno con un `.train` y un `.test`. Compruebo."
   ],
   "id": "7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_folder = './exercise_1_data'\n",
    "archivos = sorted(os.listdir(data_folder))\n",
    "\n",
    "print(\"Total de archivos en la carpeta: \" + str(len(archivos)))\n",
    "print()\n",
    "\n",
    "nombres_datasets = sorted(set(\n",
    "    f.replace('.train', '').replace('.test', '') for f in archivos\n",
    "    if f.endswith('.train') or f.endswith('.test')\n",
    "))\n",
    "\n",
    "print(\"Datasets encontrados (\" + str(len(nombres_datasets)) + \"):\")\n",
    "for i, nombre in enumerate(nombres_datasets, 1):\n",
    "    tiene_train = nombre + '.train' in archivos\n",
    "    tiene_test = nombre + '.test' in archivos\n",
    "    estado = \"completo\" if (tiene_train and tiene_test) else \"INCOMPLETO\"\n",
    "    print(\"  \" + str(i) + \". \" + nombre.ljust(20) + \" [\" + estado + \"]\")"
   ],
   "id": "8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de un archivo individual",
    "",
    "Antes de montar funciones sofisticadas, empiezo por lo básico: abrir un archivo y ver qué pinta tiene. Son archivos de texto con columnas separadas por espacios. La última columna es la clase (1.0 o -1.0) y el resto son features."
   ],
   "id": "9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ejemplo = np.loadtxt(os.path.join(data_folder, 'banana.train'))\n",
    "print(\"Forma del array: \" + str(ejemplo.shape))\n",
    "print(\"Primeras 5 filas:\")\n",
    "print(ejemplo[:5])\n",
    "print()\n",
    "print(\"Ultima columna (clases): \" + str(np.unique(ejemplo[:, -1])))"
   ],
   "id": "10",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Función `load_datafile`",
    "",
    "Creo una función que reciba la ruta de un archivo, lo cargue con `np.loadtxt` y devuelva por separado las features (X) y las clases (y). Me aseguro de que y sea un array 1D con `.ravel()`, que si no luego sklearn se queja más que yo un lunes por la mañana."
   ],
   "id": "11"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_datafile(filepath):\n",
    "    \"\"\"\n",
    "    Carga un archivo .train o .test y devuelve X (features) e y (clase).\n",
    "    La ultima columna es la variable objetivo.\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(filepath)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1].ravel()\n",
    "    return X, y\n",
    "\n",
    "X_train_banana, y_train_banana = load_datafile(\"./exercise_1_data/banana.train\")\n",
    "print(\"banana.train -> X: \" + str(X_train_banana.shape) + \", y: \" + str(y_train_banana.shape))\n",
    "print(\"Clases: \" + str(np.unique(y_train_banana)))"
   ],
   "id": "12",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Función `load_dataset`",
    "",
    "Subo un nivel: una función que dado el nombre base del dataset carga automáticamente el .train y el .test y lo organiza en un diccionario."
   ],
   "id": "13"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_dataset(base_path):\n",
    "    \"\"\"\n",
    "    Devuelve {\"train\": (X_train, y_train), \"test\": (X_test, y_test)}\n",
    "    \"\"\"\n",
    "    X_train, y_train = load_datafile(base_path + \".train\")\n",
    "    X_test, y_test = load_datafile(base_path + \".test\")\n",
    "    return {\"train\": (X_train, y_train), \"test\": (X_test, y_test)}\n",
    "\n",
    "data_banana = load_dataset(\"./exercise_1_data/banana\")\n",
    "print(\"banana -> train: \" + str(data_banana[\"train\"][0].shape) + \", test: \" + str(data_banana[\"test\"][0].shape))"
   ],
   "id": "14",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Función `load_datasets` (carga masiva)",
    "",
    "Recorro la carpeta, identifico todos los datasets y los cargo de golpe en un diccionario de diccionarios. Copiar y pegar 13 veces lo mismo no es trabajar, es sufrir."
   ],
   "id": "15"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_datasets(folder='./exercise_1_data'):\n",
    "    \"\"\"Carga todos los datasets. Devuelve dict indexado por nombre.\"\"\"\n",
    "    archivos = os.listdir(folder)\n",
    "    nombres = sorted(set(\n",
    "        f.replace('.train', '').replace('.test', '')\n",
    "        for f in archivos if f.endswith('.train')\n",
    "    ))\n",
    "    datasets = {}\n",
    "    for nombre in nombres:\n",
    "        base_path = os.path.join(folder, nombre)\n",
    "        if os.path.exists(base_path + '.train') and os.path.exists(base_path + '.test'):\n",
    "            datasets[nombre] = load_dataset(base_path)\n",
    "    return datasets\n",
    "\n",
    "datasets = load_datasets()\n",
    "\n",
    "print(\"Datasets cargados: \" + str(len(datasets)))\n",
    "print()\n",
    "for nombre, data in datasets.items():\n",
    "    X_tr = data['train'][0]\n",
    "    X_te = data['test'][0]\n",
    "    info = nombre.ljust(20) + ' -> train: ' + str(X_tr.shape[0]).rjust(5)\n",
    "    info += ' x ' + str(X_tr.shape[1]).rjust(2) + ' | test: ' + str(X_te.shape[0]).rjust(5)\n",
    "    print(info)"
   ],
   "id": "16",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el objeto `datasets` en un pickle para reutilizarlo en otros ejercicios."
   ],
   "id": "17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('datasets.pkl', 'wb') as f:\n",
    "    pickle.dump(datasets, f)\n",
    "print(\"datasets.pkl guardado.\")"
   ],
   "id": "18",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selección de clasificadores base",
    "",
    "Elijo 4 algoritmos de familias bien distintas entre sí (nada de ensembles tipo Random Forest):",
    "",
    "- **Logistic Regression (lr):** modelo lineal, rápido y estable. El clásico que nunca falla.",
    "- **K-Nearest Neighbors (knn):** basado en distancias, no paramétrico. Se adapta bien a fronteras no lineales.",
    "- **Decision Tree (dt):** basado en reglas, inestable por naturaleza. Justo lo que necesito para que el bagging brille.",
    "- **SVM con kernel RBF (svm):** potente para fronteras complejas, aunque más lento.",
    "",
    "La diversidad importa: si todos los modelos fuesen parecidos, combinarlos no aportaría nada. Necesito que se equivoquen en cosas distintas para que el ensemble compense los errores."
   ],
   "id": "19"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "base_learners = [\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5)),\n",
    "    (\"dt\", DecisionTreeClassifier(random_state=42)),\n",
    "    (\"svm\", SVC(kernel=\"rbf\", random_state=42))\n",
    "]\n",
    "\n",
    "print(\"Base learners seleccionados:\")\n",
    "for nombre, modelo in base_learners:\n",
    "    print(\"  \" + nombre + \": \" + modelo.__class__.__name__)"
   ],
   "id": "20",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento y evaluación de los modelos base",
    "",
    "Entreno cada modelo en cada dataset y mido el accuracy en test. Doble bucle y au, aquí en Murcia somos eficientes."
   ],
   "id": "21"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base_scores = pd.DataFrame(index=sorted(datasets.keys()))\n",
    "\n",
    "for nombre_modelo, modelo in base_learners:\n",
    "    accs = []\n",
    "    for ds_name in sorted(datasets.keys()):\n",
    "        X_train, y_train = datasets[ds_name]['train']\n",
    "        X_test, y_test = datasets[ds_name]['test']\n",
    "        clf = clone(modelo)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accs.append(accuracy_score(y_test, y_pred))\n",
    "    base_scores[nombre_modelo] = accs\n",
    "\n",
    "base_scores = base_scores.round(4)\n",
    "print(\"Accuracies de los modelos base:\")\n",
    "display(base_scores)\n",
    "print()\n",
    "print(\"Media por modelo:\")\n",
    "print(base_scores.mean().round(4).sort_values(ascending=False))"
   ],
   "id": "22",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualización de resultados base",
    "",
    "Pinto las accuracies para ver de un vistazo qué modelo gana en cada dataset."
   ],
   "id": "23"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "base_scores.plot(kind='bar', ax=ax, color=ROSA4, edgecolor='white', width=0.75)\n",
    "ax.set_title('Accuracy de los modelos base por dataset', fontsize=14, fontweight='bold', color='#880E4F')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.legend(title='Modelo', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_ylim(0.4, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ganador por dataset:\")\n",
    "for ds in base_scores.index:\n",
    "    winner = base_scores.loc[ds].idxmax()\n",
    "    val = base_scores.loc[ds].max()\n",
    "    print(\"  \" + ds.ljust(20) + \" -> \" + winner + \" (\" + str(round(val, 4)) + \")\")"
   ],
   "id": "24",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones rápidas: SVM gana en media, el Decision Tree destaca en image y thyroid (fronteras complejas), y LR aguanta bien en muchos datasets porque al fin y al cabo es un modelo robusto y generalista. No hay un ganador universal, que es justo lo que dice el Teorema de No Free Lunch."
   ],
   "id": "25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ensembles con Bagging",
    "",
    "Bagging (Bootstrap Aggregating): entreno 50 copias del mismo modelo, cada una con una muestra bootstrap distinta del training set, y combino por votación mayoritaria.",
    "",
    "La gracia del bagging es que si el modelo base es inestable (como el Decision Tree, que cambia mucho con pequeñas variaciones en los datos), cada copia se equivocará en cosas distintas y al promediar se reduce la varianza del error.",
    "",
    "Uso `n_jobs=-1` para aprovechar todos los cores. Si el portátil echa humo es que está trabajando, como mi abuela con el caldero de arroz."
   ],
   "id": "26"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_scores = pd.DataFrame(index=sorted(datasets.keys()))\n",
    "\n",
    "for nombre_modelo, modelo in base_learners:\n",
    "    accs = []\n",
    "    for ds_name in sorted(datasets.keys()):\n",
    "        X_train, y_train = datasets[ds_name]['train']\n",
    "        X_test, y_test = datasets[ds_name]['test']\n",
    "        bag = BaggingClassifier(\n",
    "            estimator=clone(modelo),\n",
    "            n_estimators=50,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        bag.fit(X_train, y_train)\n",
    "        y_pred = bag.predict(X_test)\n",
    "        accs.append(accuracy_score(y_test, y_pred))\n",
    "    bagging_scores[nombre_modelo + '_bag'] = accs\n",
    "\n",
    "bagging_scores = bagging_scores.round(4)\n",
    "print(\"Accuracies con Bagging:\")\n",
    "display(bagging_scores)"
   ],
   "id": "27",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparativa: base vs bagging"
   ],
   "id": "28"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_scores = pd.concat([base_scores, bagging_scores], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "all_scores.plot(kind='bar', ax=ax, color=ROSA8, edgecolor='white', width=0.85)\n",
    "ax.set_title('Base vs Bagging: accuracy por dataset', fontsize=14, fontweight='bold', color='#880E4F')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.legend(title='Modelo', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\n",
    "ax.set_ylim(0.4, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "29",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tabla de mejora: bagging vs base\n",
    "print(\"Comparativa de medias (base vs bagging):\")\n",
    "print(\"=\" * 55)\n",
    "for nombre_modelo, _ in base_learners:\n",
    "    media_base = base_scores[nombre_modelo].mean()\n",
    "    media_bag = bagging_scores[nombre_modelo + '_bag'].mean()\n",
    "    diff = media_bag - media_base\n",
    "    flecha = \"sube\" if diff > 0 else \"baja\" if diff < 0 else \"igual\"\n",
    "    linea = '  ' + nombre_modelo.ljust(5) + ': '\n",
    "    linea += str(round(media_base, 4)) + ' -> ' + str(round(media_bag, 4))\n",
    "    linea += ' (' + ('+' if diff >= 0 else '') + str(round(diff, 4)) + ', ' + flecha + ')'\n",
    "    print(linea)"
   ],
   "id": "30",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es coherente con la teoría:",
    "",
    "- El **Decision Tree** es el que más mejora con bagging (+0.036 de media). Es un modelo inestable por naturaleza: pequeños cambios en los datos producen árboles muy diferentes. Eso es exactamente lo que bagging necesita para generar diversidad real.",
    "- **KNN** y **SVM** mejoran ligeramente. Son modelos más estables, así que las 50 copias son bastante parecidas entre sí. La diversidad es baja y el ensemble apenas aporta.",
    "- **Logistic Regression** prácticamente no cambia. Es el modelo más estable de todos: le das datos distintos y sigue trazando la misma recta. 50 copias de LR son 50 veces lo mismo.",
    "",
    "Conclusión parcial: bagging funciona bien cuando el modelo base es inestable. Con modelos estables es tirar recursos de computación."
   ],
   "id": "31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ensembles con Boosting (AdaBoost)",
    "",
    "Boosting entrena modelos secuencialmente: cada nuevo modelo se centra en los ejemplos que los anteriores han fallado, reponderándolos con más peso. Es como un profe particular que insiste en los temas que peor llevas.",
    "",
    "Uso `AdaBoostClassifier` con `algorithm='SAMME'` porque el SAMME.R por defecto no funciona con todos los estimadores.",
    "",
    "**¿Por qué no hay `n_jobs` en AdaBoost?** Porque el boosting es secuencial por diseño. Cada modelo necesita los resultados del anterior para saber qué ejemplos reponderar. No se puede paralelizar. Esto es la diferencia fundamental con bagging, donde los modelos son independientes y sí se entrenan en paralelo.",
    "",
    "Importante: no todos los clasificadores son compatibles con AdaBoost. Necesita que el estimador base soporte `sample_weight` en `.fit()`. KNN no lo soporta, así que lo capturo con un try/except y sigo."
   ],
   "id": "32"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Para boosting, uso un DT poco profundo (weak learner) que es lo ideal\n",
    "boost_learners = [\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5)),\n",
    "    (\"dt\", DecisionTreeClassifier(max_depth=3, random_state=42)),\n",
    "    (\"svm\", SVC(random_state=42)),\n",
    "]\n",
    "\n",
    "boosting_scores = pd.DataFrame(index=sorted(datasets.keys()))\n",
    "modelos_excluidos = []\n",
    "\n",
    "for nombre_modelo, modelo in boost_learners:\n",
    "    accs = []\n",
    "    compatible = True\n",
    "    for ds_name in sorted(datasets.keys()):\n",
    "        X_train, y_train = datasets[ds_name]['train']\n",
    "        X_test, y_test = datasets[ds_name]['test']\n",
    "        try:\n",
    "            boost = AdaBoostClassifier(\n",
    "                estimator=clone(modelo),\n",
    "                n_estimators=50,\n",
    "                algorithm='SAMME',\n",
    "                random_state=42\n",
    "            )\n",
    "            boost.fit(X_train, y_train)\n",
    "            y_pred = boost.predict(X_test)\n",
    "            accs.append(accuracy_score(y_test, y_pred))\n",
    "        except Exception as e:\n",
    "            compatible = False\n",
    "            print(\"  \" + nombre_modelo + \" no compatible con AdaBoost: \" + str(type(e).__name__))\n",
    "            modelos_excluidos.append(nombre_modelo)\n",
    "            break\n",
    "\n",
    "    if compatible:\n",
    "        boosting_scores[nombre_modelo + '_boo'] = accs\n",
    "    else:\n",
    "        print(\"  -> Salto \" + nombre_modelo + \" para boosting.\")\n",
    "\n",
    "boosting_scores = boosting_scores.round(4)\n",
    "print()\n",
    "print(\"Accuracies con Boosting:\")\n",
    "display(boosting_scores)"
   ],
   "id": "33",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del comportamiento del boosting",
    "",
    "Aquí hay cosas muy interesantes que comentar:",
    "",
    "**¿Por qué KNN no es compatible con AdaBoost?** Porque AdaBoost necesita que el estimador base acepte pesos de muestra (`sample_weight`) en su método `.fit()` para poder reponderar los ejemplos difíciles. KNN no implementa esa funcionalidad: clasifica por cercanía y punto, no admite que le digan \"oye, este ejemplo pesa más que aquel\".",
    "",
    "**¿Por qué SVM se hunde en boosting?** Esto es lo más llamativo de los resultados. SVM con kernel RBF es un modelo fuerte y estable: ya por sí solo consigue un accuracy alto. El problema es que AdaBoost está diseñado para combinar *weak learners* (modelos simples que son solo un poco mejores que el azar). Cuando le metes un modelo fuerte como SVM pasan dos cosas: (1) los pesos de rebalanceo se vuelven muy extremos porque el modelo \"casi acierta todo\" y las pocas muestras que falla se sobrepesan de forma desproporcionada, y (2) el algoritmo SAMME tiene problemas numéricos cuando el error del base learner es muy bajo o muy alto. El resultado es que en vez de mejorar, destroza al modelo original. En banana pasa de 0.87 a 0.44, que es peor que tirar una moneda.",
    "",
    "**El Decision Tree en cambio mejora notablemente.** Un DT con max_depth=3 es un weak learner perfecto para boosting: acierta más que el azar pero comete suficientes errores como para que el rebalanceo tenga sentido. Es la combinación ideal."
   ],
   "id": "34"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_scores = pd.concat([all_scores, boosting_scores], axis=1)\n",
    "\n",
    "# Genero paleta pastel para todos los modelos\n",
    "import colorsys\n",
    "from matplotlib.colors import to_hex\n",
    "\n",
    "def paleta_rosa_n(n):\n",
    "    colores = []\n",
    "    for i in range(n):\n",
    "        # Voy de rosa a lila a azul pastel\n",
    "        hue = 0.85 + (i / n) * 0.45  # rango de tonos rosa-lila-azul\n",
    "        if hue > 1.0:\n",
    "            hue -= 1.0\n",
    "        rgb = colorsys.hls_to_rgb(hue, 0.75, 0.5)\n",
    "        colores.append(to_hex(rgb))\n",
    "    return colores\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "paleta = paleta_rosa_n(len(all_scores.columns))\n",
    "all_scores.plot(kind='bar', ax=ax, color=paleta, edgecolor='white', width=0.88)\n",
    "ax.set_title('Base vs Bagging vs Boosting', fontsize=14, fontweight='bold', color='#880E4F')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.legend(title='Modelo', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "ax.set_ylim(0.4, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Medias globales hasta ahora:\")\n",
    "print(all_scores.mean().sort_values(ascending=False).round(4))"
   ],
   "id": "35",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ensembles heterogéneos: Voting y Stacking",
    "",
    "Hasta ahora he combinado copias del mismo modelo (ensembles homogéneos). Ahora mezclo modelos distintos:",
    "",
    "- **Voting:** cada modelo vota una clase y gana la mayoría. Es una democracia de algoritmos.",
    "- **Stacking:** cada modelo hace su predicción y un meta-modelo (en mi caso una Logistic Regression) aprende a combinar esas predicciones de la mejor forma. Es como tener un jefe que sabe a quién hacer caso según el tema.",
    "",
    "`mixing_scores` tiene solo 2 columnas porque voting y stacking son técnicas que combinan todos los base learners a la vez en un único ensemble. No hay una versión \"por modelo\" como en bagging o boosting."
   ],
   "id": "36"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "mixing_scores = pd.DataFrame(index=sorted(datasets.keys()))\n",
    "\n",
    "# --- VOTING ---\n",
    "accs_vote = []\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    X_train, y_train = datasets[ds_name]['train']\n",
    "    X_test, y_test = datasets[ds_name]['test']\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[(n, clone(m)) for n, m in base_learners],\n",
    "        voting='hard', n_jobs=-1\n",
    "    )\n",
    "    voting.fit(X_train, y_train)\n",
    "    y_pred = voting.predict(X_test)\n",
    "    accs_vote.append(accuracy_score(y_test, y_pred))\n",
    "mixing_scores['vote'] = accs_vote\n",
    "\n",
    "# --- STACKING ---\n",
    "accs_stack = []\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    X_train, y_train = datasets[ds_name]['train']\n",
    "    X_test, y_test = datasets[ds_name]['test']\n",
    "    stacking = StackingClassifier(\n",
    "        estimators=[(n, clone(m)) for n, m in base_learners],\n",
    "        final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "        cv=5, n_jobs=-1\n",
    "    )\n",
    "    stacking.fit(X_train, y_train)\n",
    "    y_pred = stacking.predict(X_test)\n",
    "    accs_stack.append(accuracy_score(y_test, y_pred))\n",
    "mixing_scores['stack'] = accs_stack\n",
    "\n",
    "mixing_scores = mixing_scores.round(4)\n",
    "print(\"Accuracies con Voting y Stacking:\")\n",
    "display(mixing_scores)"
   ],
   "id": "37",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Análisis final: ranking, ganadores y conclusiones"
   ],
   "id": "38"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_scores = pd.concat([base_scores, bagging_scores, boosting_scores, mixing_scores], axis=1)\n",
    "\n",
    "print(\"GANADOR POR DATASET:\")\n",
    "print(\"=\" * 55)\n",
    "winners = all_scores.idxmax(axis=1)\n",
    "for ds in winners.index:\n",
    "    winner = winners[ds]\n",
    "    val = all_scores.loc[ds, winner]\n",
    "    print(\"  \" + ds.ljust(20) + \" -> \" + winner.ljust(12) + \" (\" + str(round(val, 4)) + \")\")\n",
    "\n",
    "print()\n",
    "ranking = all_scores.mean().sort_values(ascending=False).round(4)\n",
    "print(\"RANKING GLOBAL (media de accuracy en los 13 datasets):\")\n",
    "print(\"=\" * 55)\n",
    "for i, (modelo, media) in enumerate(ranking.items(), 1):\n",
    "    print(\"  \" + str(i).rjust(2) + \". \" + modelo.ljust(14) + \" \" + str(media))"
   ],
   "id": "39",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grafico de victorias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "wins = winners.value_counts()\n",
    "wins.plot(kind='bar', ax=axes[0], color=ROSA_SEQ[:len(wins)], edgecolor='white')\n",
    "axes[0].set_title('Victorias por modelo', fontsize=13, fontweight='bold', color='#880E4F')\n",
    "axes[0].set_ylabel('Datasets ganados')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "for i, v in enumerate(wins.values):\n",
    "    axes[0].text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Ranking de medias\n",
    "colores_rank = []\n",
    "mediana_rank = ranking.median()\n",
    "for v in ranking.values:\n",
    "    if v >= ranking.values[0] - 0.005:\n",
    "        colores_rank.append('#EC407A')\n",
    "    elif v >= mediana_rank:\n",
    "        colores_rank.append('#F48FB1')\n",
    "    else:\n",
    "        colores_rank.append('#F8BBD0')\n",
    "ranking.plot(kind='barh', ax=axes[1], color=colores_rank, edgecolor='white')\n",
    "axes[1].set_title('Ranking por accuracy media', fontsize=13, fontweight='bold', color='#880E4F')\n",
    "axes[1].set_xlabel('Accuracy media')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "40",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Heatmap completo\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "rosa_cmap = LinearSegmentedColormap.from_list('rosa',\n",
    "    ['#FFFFFF', '#FCE4EC', '#F8BBD0', '#F48FB1', '#EC407A', '#C2185B'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "sns.heatmap(all_scores, annot=True, fmt='.3f', cmap=rosa_cmap, ax=ax,\n",
    "            linewidths=0.5, linecolor='white', vmin=0.5, vmax=1.0)\n",
    "ax.set_title('Accuracy de todos los modelos en todos los datasets',\n",
    "             fontsize=13, fontweight='bold', color='#880E4F')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "41",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de diversidad entre modelos base",
    "",
    "Para entender por qué los ensembles heterogéneos (voting, stacking) funcionan bien, voy a medir cuánto \"discrepan\" los modelos base entre sí. Si todos predicen lo mismo, combinarlos no aporta nada. Si discrepan mucho en sus errores, el ensemble puede compensarlos.",
    "",
    "Calculo el porcentaje de desacuerdo entre cada par de modelos: en cuántos ejemplos de test uno acierta y el otro falla (o viceversa)."
   ],
   "id": "42"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mido la discrepancia (disagreement) entre pares de modelos\n",
    "# en cada dataset, luego promedio\n",
    "from itertools import combinations\n",
    "\n",
    "pares = list(combinations([n for n, _ in base_learners], 2))\n",
    "disagreement = pd.DataFrame(index=sorted(datasets.keys()), columns=[p[0]+'_vs_'+p[1] for p in pares])\n",
    "\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    X_train, y_train = datasets[ds_name]['train']\n",
    "    X_test, y_test = datasets[ds_name]['test']\n",
    "    preds = {}\n",
    "    for nombre_modelo, modelo in base_learners:\n",
    "        clf = clone(modelo)\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds[nombre_modelo] = clf.predict(X_test)\n",
    "    for (m1, m2) in pares:\n",
    "        disag = (preds[m1] != preds[m2]).mean()\n",
    "        disagreement.loc[ds_name, m1 + '_vs_' + m2] = round(disag, 4)\n",
    "\n",
    "disagreement = disagreement.astype(float)\n",
    "print(\"Porcentaje de desacuerdo entre pares de modelos (media en todos los datasets):\")\n",
    "print(\"=\" * 55)\n",
    "for col in disagreement.columns:\n",
    "    media = disagreement[col].mean()\n",
    "    print(\"  \" + col.ljust(12) + \": \" + str(round(media * 100, 1)) + \"%\")\n",
    "\n",
    "print()\n",
    "print(\"Cuanto mayor sea el desacuerdo, mas potencial tiene el ensemble para mejorar.\")\n",
    "print(\"Si dos modelos siempre predicen lo mismo, combinarlos no aporta nada nuevo.\")"
   ],
   "id": "43",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos que más discrepan entre sí son los que más se benefician de ser combinados. Esto explica por qué el stacking (que aprende a combinar las predicciones de los 4 modelos) consigue el mejor ranking global: aprovecha tanto la precisión individual como la diversidad entre modelos."
   ],
   "id": "44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones",
    "",
    "1. **No hay un modelo universal.** Depende del dataset. SVM gana en unos, DT en otros, LR en otros. Es el Teorema de No Free Lunch en estado puro.",
    "",
    "2. **Bagging mejora a los inestables.** El Decision Tree sube un 3.6% de media con bagging. LR y SVM apenas se inmutan porque son modelos estables: 50 copias de lo mismo siguen siendo lo mismo.",
    "",
    "3. **Boosting necesita weak learners.** Con DT poco profundo funciona genial, pero con SVM se va al garete porque es un modelo demasiado fuerte. AdaBoost no sabe qué hacer con un estimador que ya acierta casi todo. KNN directamente no es compatible porque no soporta sample_weight.",
    "",
    "4. **Voting y stacking son consistentes.** Rara vez ganan en un dataset concreto, pero rara vez son los peores. El stacking es el ganador del ranking global porque tiene un meta-modelo que aprende a combinar las fortalezas de cada clasificador.",
    "",
    "5. **La diversidad es la clave.** He medido el desacuerdo entre modelos y los pares que más discrepan son los que más se benefician de ser combinados. Sin diversidad, no hay ensemble que valga."
   ],
   "id": "45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Ejercicio avanzado: mi propio super-ensemble",
    "",
    "Voy a construir un \"ensemble de ensembles\": un stacking donde los estimadores base ya son ensembles:",
    "",
    "- **Componente 1:** BaggingClassifier de árboles (captura la inestabilidad del DT)",
    "- **Componente 2:** AdaBoostClassifier de stumps (se centra en los ejemplos difíciles)",
    "- **Componente 3:** VotingClassifier heterogéneo (diversidad de familias)",
    "",
    "Un meta-modelo de Logistic Regression decide cómo ponderarlos. Es el típico movimiento murciano: si el plato ya está bueno, le echamos más cosas. A veces funciona, a veces te pasas con la ñora."
   ],
   "id": "46"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "advanced_scores = pd.DataFrame(index=sorted(datasets.keys()))\n",
    "accs_super = []\n",
    "\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    X_train, y_train = datasets[ds_name]['train']\n",
    "    X_test, y_test = datasets[ds_name]['test']\n",
    "\n",
    "    bag_tree = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=42),\n",
    "        n_estimators=50, random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    ada_stump = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=2, random_state=42),\n",
    "        n_estimators=50, algorithm='SAMME', random_state=42\n",
    "    )\n",
    "\n",
    "    voter = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"lr\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "            (\"knn\", KNeighborsClassifier(n_neighbors=5)),\n",
    "            (\"svm\", SVC(random_state=42))\n",
    "        ],\n",
    "        voting='hard', n_jobs=-1\n",
    "    )\n",
    "\n",
    "    super_ensemble = StackingClassifier(\n",
    "        estimators=[\n",
    "            (\"bag_tree\", bag_tree),\n",
    "            (\"ada_stump\", ada_stump),\n",
    "            (\"voter\", voter)\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "        cv=5, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    super_ensemble.fit(X_train, y_train)\n",
    "    y_pred = super_ensemble.predict(X_test)\n",
    "    accs_super.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "advanced_scores['super_ensemble'] = accs_super\n",
    "advanced_scores = advanced_scores.round(4)\n",
    "\n",
    "print(\"Accuracies del super-ensemble:\")\n",
    "display(advanced_scores)\n",
    "print()\n",
    "media_s = str(round(advanced_scores['super_ensemble'].mean(), 4))\n",
    "print(\"Media global: \" + media_s)"
   ],
   "id": "47",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_final = pd.concat([all_scores, advanced_scores], axis=1)\n",
    "ranking_final = all_final.mean().sort_values(ascending=False).round(4)\n",
    "\n",
    "print(\"RANKING FINAL DEFINITIVO:\")\n",
    "print(\"=\" * 55)\n",
    "for i, (modelo, media) in enumerate(ranking_final.items(), 1):\n",
    "    marca = \"  <-- mi criatura\" if modelo == \"super_ensemble\" else \"\"\n",
    "    print(\"  \" + str(i).rjust(2) + \". \" + modelo.ljust(18) + \" \" + str(media) + marca)\n",
    "\n",
    "print()\n",
    "wins_s = int((all_final.idxmax(axis=1) == 'super_ensemble').sum())\n",
    "total = len(all_final)\n",
    "print(\"El super-ensemble gana en \" + str(wins_s) + \" de \" + str(total) + \" datasets.\")"
   ],
   "id": "48",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Heatmap definitivo\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "sns.heatmap(all_final, annot=True, fmt='.3f', cmap=rosa_cmap, ax=ax,\n",
    "            linewidths=0.5, linecolor='white', vmin=0.5, vmax=1.0)\n",
    "ax.set_title('Mapa de calor definitivo: todos los modelos y ensembles',\n",
    "             fontsize=14, fontweight='bold', color='#880E4F')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"13 datasets, un monton de modelos, y la conclusion de siempre:\")\n",
    "print(\"juntos somos mas fuertes que solos. Pero hay que saber juntarse bien.\")"
   ],
   "id": "49",
   "execution_count": null,
   "outputs": []
  }
 ]
}